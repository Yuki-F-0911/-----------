"""
Webæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
X(Twitter), Redditã®æŠ•ç¨¿ã‚’Webæ¤œç´¢çµŒç”±ã§å–å¾—

X/Twitter APIãŒä¸è¦ã§ã€Serper APIã®ã¿ã§å‹•ä½œ
"""

import re
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import requests
from config import SERPER_API_KEY, GOOGLE_SEARCH_API_KEY, GOOGLE_SEARCH_ENGINE_ID


@dataclass
class SocialPost:
    """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿"""
    platform: str  # twitter, reddit, etc.
    title: str
    url: str
    snippet: str
    author: str = ''
    post_type: str = ''  # tweet, reddit_post, etc.

    def to_dict(self):
        return asdict(self)


def search_serper(query: str, num_results: int = 10) -> List[Dict]:
    """Serper APIã§æ¤œç´¢"""
    if not SERPER_API_KEY:
        print('âš ï¸ SERPER_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“')
        return []

    try:
        response = requests.post(
            'https://google.serper.dev/search',
            headers={
                'Content-Type': 'application/json',
                'X-API-KEY': SERPER_API_KEY,
            },
            json={
                'q': query,
                'num': num_results,
                'gl': 'jp',
                'hl': 'ja',
            },
            timeout=30
        )
        response.raise_for_status()
        data = response.json()
        return data.get('organic', [])
    except Exception as e:
        print(f'âŒ Serperæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}')
        return []


def extract_twitter_username(url: str) -> str:
    """URLã‹ã‚‰Twitterãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º"""
    patterns = [
        r'(?:twitter\.com|x\.com)/([^/]+)/status',
        r'(?:twitter\.com|x\.com)/([^/]+)',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            username = match.group(1)
            if username not in ['search', 'hashtag', 'i', 'intent']:
                return username
    return ''


def extract_reddit_info(url: str) -> Dict:
    """URLã‹ã‚‰Redditæƒ…å ±ã‚’æŠ½å‡º"""
    info = {'subreddit': '', 'type': 'post'}
    
    # r/subreddit/comments/... ãƒ‘ã‚¿ãƒ¼ãƒ³
    match = re.search(r'reddit\.com/r/([^/]+)', url)
    if match:
        info['subreddit'] = match.group(1)
    
    if '/comments/' in url:
        info['type'] = 'post'
    elif '/r/' in url:
        info['type'] = 'subreddit'
    
    return info


def search_twitter_posts(
    query: str,
    max_results: int = 10,
    lang: str = 'ja',
) -> List[SocialPost]:
    """
    Webæ¤œç´¢çµŒç”±ã§X(Twitter)ã®æŠ•ç¨¿ã‚’æ¤œç´¢
    Twitter APIãªã—ã§å‹•ä½œ
    """
    # site:twitter.com OR site:x.com ã§æ¤œç´¢
    search_query = f'{query} (site:twitter.com OR site:x.com)'
    
    results = search_serper(search_query, max_results * 2)
    
    posts = []
    seen_urls = set()
    
    for result in results:
        url = result.get('link', '')
        
        # Twitter/X ã®URLã®ã¿
        if not ('twitter.com' in url or 'x.com' in url):
            continue
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in seen_urls:
            continue
        seen_urls.add(url)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒšãƒ¼ã‚¸ï¼ˆå®Ÿéš›ã®ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰ã®ã¿
        if '/status/' not in url:
            continue
        
        username = extract_twitter_username(url)
        title = result.get('title', '')
        snippet = result.get('snippet', '')
        
        posts.append(SocialPost(
            platform='twitter',
            title=title,
            url=url,
            snippet=snippet,
            author=f'@{username}' if username else '',
            post_type='tweet',
        ))
        
        if len(posts) >= max_results:
            break
    
    return posts


def search_reddit_posts_via_web(
    query: str,
    max_results: int = 10,
    subreddits: Optional[List[str]] = None,
) -> List[SocialPost]:
    """
    Webæ¤œç´¢çµŒç”±ã§Redditã®æŠ•ç¨¿ã‚’æ¤œç´¢
    Reddit APIãªã—ã§å‹•ä½œ
    """
    # site:reddit.com ã§æ¤œç´¢
    if subreddits:
        subreddit_query = ' OR '.join([f'site:reddit.com/r/{s}' for s in subreddits])
        search_query = f'{query} ({subreddit_query})'
    else:
        search_query = f'{query} site:reddit.com'
    
    results = search_serper(search_query, max_results * 2)
    
    posts = []
    seen_urls = set()
    
    for result in results:
        url = result.get('link', '')
        
        # Reddit ã®URLã®ã¿
        if 'reddit.com' not in url:
            continue
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in seen_urls:
            continue
        seen_urls.add(url)
        
        # å®Ÿéš›ã®æŠ•ç¨¿ãƒšãƒ¼ã‚¸ã®ã¿
        if '/comments/' not in url:
            continue
        
        reddit_info = extract_reddit_info(url)
        title = result.get('title', '')
        snippet = result.get('snippet', '')
        
        posts.append(SocialPost(
            platform='reddit',
            title=title,
            url=url,
            snippet=snippet,
            author=f'r/{reddit_info["subreddit"]}' if reddit_info['subreddit'] else '',
            post_type='reddit_post',
        ))
        
        if len(posts) >= max_results:
            break
    
    return posts


def search_shoe_reviews_social(
    brand: str,
    model_name: str,
    max_results: int = 10,
    platforms: Optional[List[str]] = None,
) -> Dict[str, List[SocialPost]]:
    """
    ã‚·ãƒ¥ãƒ¼ã‚ºã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã‹ã‚‰æ¤œç´¢
    
    Args:
        brand: ãƒ–ãƒ©ãƒ³ãƒ‰å
        model_name: ãƒ¢ãƒ‡ãƒ«å
        max_results: å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®æœ€å¤§çµæœæ•°
        platforms: æ¤œç´¢å¯¾è±¡ ['twitter', 'reddit']
    
    Returns:
        ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã”ã¨ã®æŠ•ç¨¿ãƒªã‚¹ãƒˆ
    """
    if platforms is None:
        platforms = ['twitter', 'reddit']
    
    results = {}
    
    queries = [
        f'{brand} {model_name} ãƒ¬ãƒ“ãƒ¥ãƒ¼',
        f'{brand} {model_name} review',
    ]
    
    if 'twitter' in platforms:
        print('ğŸ¦ X(Twitter) æ¤œç´¢ä¸­...')
        twitter_posts = []
        seen = set()
        for query in queries:
            posts = search_twitter_posts(query, max_results=max_results)
            for post in posts:
                if post.url not in seen:
                    seen.add(post.url)
                    twitter_posts.append(post)
        results['twitter'] = twitter_posts[:max_results]
        print(f'   {len(results["twitter"])} ä»¶å–å¾—')
    
    if 'reddit' in platforms:
        print('ğŸ“ Reddit æ¤œç´¢ä¸­...')
        running_subreddits = ['running', 'RunningShoeGeeks', 'AdvancedRunning']
        reddit_posts = []
        seen = set()
        for query in queries:
            posts = search_reddit_posts_via_web(
                query, 
                max_results=max_results,
                subreddits=running_subreddits
            )
            for post in posts:
                if post.url not in seen:
                    seen.add(post.url)
                    reddit_posts.append(post)
        results['reddit'] = reddit_posts[:max_results]
        print(f'   {len(results["reddit"])} ä»¶å–å¾—')
    
    return results


def search_general_running_social(max_results: int = 20) -> Dict[str, List[SocialPost]]:
    """
    ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚·ãƒ¥ãƒ¼ã‚ºé–¢é€£ã®æŠ•ç¨¿ã‚’åºƒãæ¤œç´¢
    """
    queries = [
        'ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚·ãƒ¥ãƒ¼ã‚º ãƒ¬ãƒ“ãƒ¥ãƒ¼',
        'ãƒãƒ©ã‚½ãƒ³ã‚·ãƒ¥ãƒ¼ã‚º ãŠã™ã™ã‚ 2024',
        'running shoes review',
    ]
    
    all_twitter = []
    all_reddit = []
    seen_twitter = set()
    seen_reddit = set()
    
    for query in queries:
        # Twitter
        posts = search_twitter_posts(query, max_results=10)
        for post in posts:
            if post.url not in seen_twitter:
                seen_twitter.add(post.url)
                all_twitter.append(post)
        
        # Reddit
        posts = search_reddit_posts_via_web(query, max_results=10)
        for post in posts:
            if post.url not in seen_reddit:
                seen_reddit.add(post.url)
                all_reddit.append(post)
    
    return {
        'twitter': all_twitter[:max_results],
        'reddit': all_reddit[:max_results],
    }


if __name__ == '__main__':
    print('=== Webæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«åé›†ãƒ†ã‚¹ãƒˆ ===\n')
    
    if not SERPER_API_KEY:
        print('âŒ SERPER_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“')
        print('   .env.local ã« SERPER_API_KEY ã‚’è¿½åŠ ã—ã¦ãã ã•ã„')
        exit(1)
    
    # ç‰¹å®šã‚·ãƒ¥ãƒ¼ã‚ºã®æ¤œç´¢
    print('ğŸ” Nike Pegasus 41 ã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«æŠ•ç¨¿:\n')
    results = search_shoe_reviews_social('Nike', 'Pegasus 41', max_results=5)
    
    if results.get('twitter'):
        print('\nğŸ¦ X (Twitter):')
        for post in results['twitter']:
            print(f'   {post.author}: {post.title[:50]}...')
            print(f'   URL: {post.url}')
            print()
    
    if results.get('reddit'):
        print('\nğŸ“ Reddit:')
        for post in results['reddit']:
            print(f'   {post.author}: {post.title[:50]}...')
            print(f'   URL: {post.url}')
            print()
    
    print('\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†')

